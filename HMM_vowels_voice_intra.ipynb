{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize Phones from Speech Sentences using Hidden Markov Model\n",
    "\n",
    "## Three HMM model are built for /a/, /u/, and /i/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from python_speech_features import mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Vowel Data for Training the HMM\n",
    "Load one normal vowels (/a/ normal, /u/ normal, /i/ normal) but only using voice channels, the normal vowels are used for fitting the Hidden Markov Model, generate all hidden state distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Subject\n",
    "subject = \"R031\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract variables that contain related data\n",
    "a_normal = sio.loadmat('Data/%s/a_normal.mat' % subject)\n",
    "u_normal = sio.loadmat('Data/%s/u_normal.mat' % subject)\n",
    "i_normal = sio.loadmat('Data/%s/i_normal.mat' % subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Features from Vowels\n",
    "Load the vowels and extrac MFCC features from each repetition to construct the training vectors to train the HMM model\n",
    "\n",
    "### Mel Frequency Cepstral Coefficient (MFCC) \n",
    "1. Frame the signal into short frames.\n",
    "2. For each frame calculate the periodogram estimate of the power spectrum.\n",
    "3. Apply the mel filterbank to the power spectra, sum the energy in each filter.\n",
    "4. Take the logarithm of all filterbank energies.\n",
    "5. Take the DCT of the log filterbank energies.\n",
    "6. Keep DCT coefficients 2-13, discard the rest.\n",
    "\n",
    "Example:\n",
    "<img src=\"Figure/time_signal.jpg\" width=\"700\"/>\n",
    "<img src=\"Figure/mfcc_raw.jpg\" width=\"700\"/>\n",
    "\n",
    "#### TO-DO\n",
    "Need to align all voice signals before extracting features and train HMM models. Since the silence part of audio should not be part of the state distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_overlap = 25\n",
    "win_step    = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test script for re-implementing the feature extraction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Load training vowels and extract features\n",
    "Load 50 repetitions from normal vowels production and then extract MFCC cofficients from each vowel to train the HMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vowels_training(vowel):\n",
    "    Tmp_Data = vowel['data']\n",
    "    Start = vowel['datastart']\n",
    "    End = vowel['dataend']\n",
    "    Sample_Rate = vowel['samplerate']\n",
    "\n",
    "    rep = Start.shape[1]\n",
    "    sample_rate = Sample_Rate[4,0]\n",
    "\n",
    "    print('Total Repetitions:', rep)\n",
    "    print(\"Sampling Rate: \", sample_rate)\n",
    "\n",
    "    # Extract features from all 55 rep of voice signals\n",
    "    vowel_feature = np.empty((0, 13))\n",
    "    length = []\n",
    "    for i in range(50):\n",
    "        # Get the indices for the current repetiton\n",
    "        voice_start = int(Start[4,i]) - 1\n",
    "        voice_end = int(End[4,i])\n",
    "\n",
    "        # Extract and center the current voice signal\n",
    "        voice_sample = Tmp_Data[0,voice_start:voice_end]\n",
    "        voice_sample = voice_sample - np.mean(voice_sample)\n",
    "\n",
    "        # MFCC feature vectors are typically computed every 10ms using \n",
    "        # an overlapping analysis window of 25ms\n",
    "        mfcc_feat = mfcc(voice_sample, sample_rate, 0.001*win_overlap, 0.001*win_step)\n",
    "\n",
    "        # Concatnate individual feature into one single array\n",
    "        vowel_feature = np.append(vowel_feature, mfcc_feat, axis=0)\n",
    "        length.append(int(mfcc_feat.shape[0]))\n",
    "\n",
    "    print('Dimensionality of ONE vowel features: ', vowel_feature[0].shape)        \n",
    "    print('Total # of feature sequence: ', vowel_feature.shape[0])\n",
    "\n",
    "    return vowel_feature, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_a_tr, length_a_tr = load_vowels_training(a_normal)\n",
    "feature_u_tr, length_u_tr = load_vowels_training(u_normal)\n",
    "feature_i_tr, length_i_tr = load_vowels_training(i_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a HMM Model\n",
    "Use Gaussian distribution for estimating emission probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HMM model and estimate parameters for /a/ normal\n",
    "model_a_normal = hmm.GaussianHMM(n_components=num_components, covariance_type=\"full\", n_iter=100)\n",
    "model_a_normal.fit(feature_a_tr, length_a_tr)\n",
    "model_a_normal.monitor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HMM model and estimate parameters for /u/ normal\n",
    "model_u_normal = hmm.GaussianHMM(n_components=num_components, covariance_type=\"full\", n_iter=100)\n",
    "model_u_normal.fit(feature_u_tr, length_u_tr)\n",
    "model_u_normal.monitor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HMM model and estimate parameters for /i/ normal\n",
    "model_i_normal = hmm.GaussianHMM(n_components=num_components, covariance_type=\"full\", n_iter=100)\n",
    "model_i_normal.fit(feature_i_tr, length_i_tr)\n",
    "model_i_normal.monitor_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Do: \n",
    "Re-order the state sequence based on the variance of that particular state. The intuition here is assuming that the empty (quiet) state 0 has the smallest variance; the beginning state 1 of a vowel has the medium amount of variance; and the main vowel state 2 should have the largest variance.\n",
    "#### Question to be solved\n",
    "How to guarantee the state order could be 0,1,2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use Trainned HMM Model to Perform State Prediction on Given Voices\n",
    "Use the three HMM models which are estimated previously for /a/ normal, /u/ normal, /i/ normal to:\n",
    "1. Find corresponding states on original vowel signals for reference.\n",
    "2. Predict corresponding states for given syllables to validate the models.\n",
    "3. Predict corresponding states for given sentences to test the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Use vowels as tesing voice signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Load testing voice (vowels or speeches), extract features\n",
    "Similar to the function of loading training vowels. However, this only loads one repetition for qualitative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_voice_testing(voice, index, L):\n",
    "    Tmp_Data = voice['data']\n",
    "    Start = voice['datastart']\n",
    "    End = voice['dataend']\n",
    "    Sample_Rate = voice['samplerate']\n",
    "\n",
    "    rep = Start.shape[1]\n",
    "    sample_rate = Sample_Rate[4,0]\n",
    "\n",
    "    # Extract features from all 55 rep of voice signals\n",
    "    voice_feature = np.empty((0, 13))\n",
    "    length = []\n",
    "\n",
    "    # Get the indices for the current repetiton\n",
    "    voice_start = int(Start[4,index]) - 1\n",
    "    voice_end = int(End[4,index])\n",
    "\n",
    "    # Extract and center the current voice signal\n",
    "    voice_sample = Tmp_Data[0,voice_start:voice_end]\n",
    "    voice_sample = voice_sample - np.mean(voice_sample)\n",
    "\n",
    "    # MFCC feature vectors are typically computed every 10ms using \n",
    "    # an overlapping analysis window of 25ms\n",
    "    mfcc_feat = mfcc(voice_sample, sample_rate, 0.001*win_overlap, 0.001*win_step)\n",
    "\n",
    "    # Concatnate individual feature into one single array\n",
    "    voice_feature = np.append(voice_feature, mfcc_feat, axis=0)\n",
    "    length.append(int(mfcc_feat.shape[0]))\n",
    "\n",
    "    # Plot ONE voice sample signals    \n",
    "    plt.figure(1, figsize=(15, 6))\n",
    "    plt.plot(voice_sample)\n",
    "#     plt.ylim(-0.04, 0.04)\n",
    "    plt.xlim(L[0], L[1])\n",
    "    plt.title(\"Testing Voice\")\n",
    "    plt.show()\n",
    "    \n",
    "    return voice_sample, voice_feature, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load testing /a/ normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_a_te, feature_a_te, length_a_te = load_voice_testing(a_normal, 54, [0, 40000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Function: Restore states prediction from the feature space to the original signal space\n",
    "The voice feature vectors are extracted every 10ms using a 25ms overlapping window. Thus, each predicted state for the feature vector need to be expanded to the original voice signal in order to get the indices for the start and end for the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_vec = ['r','g','y','c','m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_voice_state(model, voice_sample, voice_feature, c, L):\n",
    "    \"\"\" THIS PARAMETER NEED TO BE FURTHER INVESTIGATED \"\"\"\n",
    "    # sample window = time x sampling rate\n",
    "    sample_window = int(0.01 * 20000)\n",
    "    \"\"\" THIS PARAMETER NEED TO BE FURTHER INVESTIGATED \"\"\"\n",
    "\n",
    "    # Use trainned HMM to predict the states\n",
    "    state_prediction = model.predict(voice_feature)\n",
    "    state_length = len(state_prediction)\n",
    "\n",
    "#     print(state_prediction)\n",
    "\n",
    "    # Expand the state prediction from feature vectors to the original\n",
    "    # voice signals\n",
    "    # i - index\n",
    "    # s - state\n",
    "    voice_state = np.zeros((num_components,voice_sample.size))\n",
    "    for i,s in enumerate(state_prediction):\n",
    "\n",
    "        # Skip the first and last state to make it able to detect \n",
    "        # after differentiation\n",
    "        # the first state\n",
    "        if i == 0:\n",
    "            for j in range(1, sample_window):\n",
    "                voice_state[s,j] = 1\n",
    "        # from the second state until the second last state\n",
    "        elif i < state_length - 1:\n",
    "            for j in range(i*sample_window, i*sample_window+sample_window):\n",
    "                voice_state[s,j] = 1\n",
    "        # last state\n",
    "        else:\n",
    "            for j in range(i*sample_window, voice_sample.size-1):\n",
    "                voice_state[s,j] = 1\n",
    "\n",
    "    # Plot the expanded voice state sequence\n",
    "    plt.figure(2, figsize=(15, 6))\n",
    "    plt.plot(voice_sample, c='b', alpha=0.8)\n",
    "\n",
    "    for s in range(0,num_components):\n",
    "        i_start = []    # all start indices for the current state\n",
    "        i_end   = []    # all end indices for the current state\n",
    "\n",
    "        # peform the differentiation\n",
    "        voice_detect = np.diff(voice_state[s,:])\n",
    "        # find non-zero elements +1/-1\n",
    "        for i, v in enumerate(voice_detect):\n",
    "            if v == 1:\n",
    "                i_start.append(i)\n",
    "            elif v == -1:\n",
    "                i_end.append(i)\n",
    "\n",
    "        for i,j in zip(i_start, i_end):\n",
    "            p = plt.axvspan(i, j, facecolor=c[s] , alpha=0.4)\n",
    "\n",
    "    plt.xlim(L[0], L[1])    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict testing /a/ normal\n",
    "State sequence order: red   ---> 0, green ---> 1, yello ---> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_voice_state(model_a_normal, voice_a_te, feature_a_te, color_vec, [0, 40000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load testing /u/ normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_u_te, feature_u_te, length_u_te = load_voice_testing(u_normal, 53, [0, 40000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict testing /u/ normal\n",
    "State sequence order: red   ---> 0, green ---> 1, yello ---> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_voice_state(model_u_normal, voice_u_te, feature_u_te, color_vec, [0, 40000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load testing /i/ normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_i_te, feature_i_te, length_i_te = load_voice_testing(i_normal, 52, [0, 40000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict testing /i/ normal\n",
    "State sequence order: red   ---> 0, green ---> 1, yello ---> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_voice_state(model_i_normal, voice_i_te, feature_i_te, color_vec, [0, 40000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Using syllable1 as tesing voice signal\n",
    "\n",
    "“afa afa afa ifi ifi ifi ufu ufu ufu”\n",
    "\n",
    "For demonstration purpose, when evaluation the states prediction returned from HMM, separate the whole syllable into three parts:\n",
    "1. \"afa afa afa\"\n",
    "2. \"ifi ifi ifi\"\n",
    "3. \"ufu ufu ufu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable = sio.loadmat('Data/%s/syllable1.mat' % subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_syl_te, feature_syl_te, length_syl_te = load_voice_testing(syllable, 0)\n",
    "predict_voice_state(model_a_normal, voice_syl_te, feature_syl_te, color_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_syl_te, feature_syl_te, length_syl_te = load_voice_testing(syllable, 0)\n",
    "predict_voice_state(model_u_normal, voice_syl_te, feature_syl_te, color_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_syl_te, feature_syl_te, length_syl_te = load_voice_testing(syllable, 0)\n",
    "predict_voice_state(model_i_normal, voice_syl_te, feature_syl_te, color_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Using sentence1 as tesing voice signal\n",
    "“The dew shimmered over my shiny blue shell again”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = sio.loadmat('Data/%s/sentence1.mat' % subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Using sentence2 as tesing voice signal\n",
    "“Only we feel you do fail in new fallen\n",
    "dew”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = sio.loadmat('Data/%s/sentence2.mat' % subject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
